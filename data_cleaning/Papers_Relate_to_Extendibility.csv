,paper_id,title,itemType,abstract,publication,year
0,P681,Analysis of Metabolomics Datasets with High-Performance Computing and Metabolite Atlases,journalArticle,"Even with the widespread use of liquid chromatography mass spectrometry (LC/MS) based metabolomics, there are still a number of challenges facing this promising technique. Many, diverse experimental workflows exist; yet there is a lack of infrastructure and systems for tracking and sharing of information. Here, we describe the Metabolite Atlas framework and interface that provides highly-efficient, web-based access to raw mass spectrometry data in concert with assertions about chemicals detected to help address some of these challenges. This integration, by design, enables experimentalists to explore their raw data, specify and refine features annotations such that they can be leveraged for future experiments. Fast queries of the data through the web using SciDB, a parallelized database for high performance computing, make this process operate quickly. By using scripting containers, such as IPython or Jupyter, to analyze the data, scientists can utilize a wide variety of freely available graphing, statistics, and information management resources. In addition, the interfaces facilitate integration with systems biology tools to ultimately link metabolomics data with biological models.",Metabolites,2015
1,P1120,BioShaDock: a community driven bioinformatics shared Docker-based tools registry,journalArticle,,F1000Research,2015
2,P2774,Enhanced reproducibility of SADI web service workflows with Galaxy and Docker.,journalArticle,"Semantic Web technologies have been widely applied in the life sciences, for example by data providers such as OpenLifeData and through web services frameworks such as SADI. The recently reported OpenLifeData2SADI project offers access to the vast OpenLifeData data store through SADI services. This article describes how to merge data retrieved from OpenLifeData2SADI with other SADI services using the Galaxy bioinformatics analysis platform, thus making this semantic data more amenable to complex analyses. This is demonstrated using a working example, which is made distributable and reproducible through a Docker image that includes SADI tools, along with the data and workflows that constitute the demonstration. The combination of Galaxy and Docker offers a solution for faithfully reproducing and sharing complex data retrieval and analysis workflows based on the SADI Semantic web service design patterns.",GigaScience,2015
3,P4186,"Integrating Containers into Workflows: A Case Study Using Makeflow, Work Queue, and Docker",conferencePaper,"Workflows are a widely used abstraction for representing large scientific applications and executing them on distributed systems such as clusters, clouds, and grids. However, workflow systems have been largely silent on the question of precisely what environment each task in the workflow is expected to run in. As a result, a workflow may run correctly in the environment in which it was designed, but when moved to another machine, is highly likely to fail due to differences in the operating system, installed applications, available data, and so forth. Lightweight container technology has recently arisen as a potential solution to this problem, by providing a well-defined execution environments at the operating system level. In this paper, we consider how to best integrate container technology into an existing workflow system, using Makeflow, Work Queue, and Docker as examples of current technology. A brief performance study of Docker shows very little overhead in CPU and I/O performance, but significant costs in creating and deleting containers. Taking this into account, we describe four different methods of connecting containers to different points of the infrastructure, and explain several methods of managing the container images that must be distributed to executing tasks. We explore the performance of a large bioinformatics workload on a Docker-enabled cluster, and observe the best configuration to be locally-managed containers that are shared between multiple tasks.",ACM,2015
4,P4609,Merging OpenLifeData with SADI services using Galaxy and Docker,journalArticle,"bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution",bioRxiv,2015
5,P5166,NGSeasy: a next generation sequencing pipeline in Docker containers,journalArticle,,F1000Research,2015
6,P6006,ReproPhylo: An Environment for Reproducible Phylogenomics,journalArticle,"The reproducibility of experiments is key to the scientific process, and particularly necessary for accurate reporting of analyses in data-rich fields such as phylogenomics. We present ReproPhylo, a phylogenomic analysis environment developed to ensure experimental reproducibility, to facilitate the handling of large-scale data, and to assist methodological experimentation. Reproducibility, and instantaneous repeatability, is built in to the ReproPhylo system and does not require user intervention or configuration because it stores the experimental workflow as a single, serialized Python object containing explicit provenance and environment information. This 'single file' approach ensures the persistence of provenance across iterations of the analysis, with changes automatically managed by the version control program Git. This file, along with a Git repository, are the primary reproducibility outputs of the program. In addition, ReproPhylo produces an extensive human-readable report and generates a comprehensive experimental archive file, both of which are suitable for submission with publications. The system facilitates thorough experimental exploration of both parameters and data. ReproPhylo is a platform independent CC0 Python module and is easily installed as a Docker image or a WinPython self-sufficient package, with a Jupyter Notebook GUI, or as a slimmer version in a Galaxy distribution.",PLOS Computational Biology,2015
7,P6223,SBMLDock: Docker Driven Systems Biology Tool Development and Usage,bookSection,,Springer International Publishing,2015
8,P2578,Dynamic Deployment of Scientific Workflows in the Cloud Using Container Virtualization,conferencePaper,"Scientific workflows are increasingly being migrated to the Cloud. However, workflow developers face the problem of which Cloud to choose and, more importantly, how to avoid vendor lock-in. This is because there are a range of Cloud platforms, each with different functionality and interfaces. In this paper we propose a solution - a system that allows workflows to be portable across a range of Clouds. This portability is achieved through a new framework for building, dynamically deploying and enacting workflows. It combines the TOSCA specification language and container-based virtualization. TOSCA is used to build a reusable and portable description of a workflow which can be automatically deployed and enacted using Docker containers. We describe a working implementation of our framework and evaluate it using a set of existing scientific workflows that illustrate the flexibility of the proposed approach.",,2016
9,P2789,"Enhancing pre-defined workflows with ad hoc analytics using Galaxy, Docker and Jupyter",journalArticle,"bioRxiv - the preprint server for biology, operated by Cold Spring Harbor Laboratory, a research and educational institution",bioRxiv,2016
10,P3619,GUIdock: Using Docker Containers with a Common Graphics User Interface to Address the Reproducibility of Research,journalArticle,"Reproducibility is vital in science. For complex computational methods, it is often necessary, not just to recreate the code, but also the software and hardware environment to reproduce results. Virtual machines, and container software such as Docker, make it possible to reproduce the exact environment regardless of the underlying hardware and operating system. However, workflows that use Graphical User Interfaces (GUIs) remain difficult to replicate on different host systems as there is no high level graphical software layer common to all platforms. GUIdock allows for the facile distribution of a systems biology application along with its graphics environment. Complex graphics based workflows, ubiquitous in systems biology, can now be easily exported and reproduced on many different platforms. GUIdock uses Docker, an open source project that provides a container with only the absolutely necessary software dependencies and configures a common X Windows (X11) graphic interface on Linux, Macintosh and Windows platforms. As proof of concept, we present a Docker package that contains a Bioconductor application written in R and C++ called networkBMA for gene network inference. Our package also includes Cytoscape, a java-based platform with a graphical user interface for visualizing and analyzing gene networks, and the CyNetworkBMA app, a Cytoscape app that allows the use of networkBMA via the user-friendly Cytoscape interface.",PLoS ONE,2016
11,P6488,Software Provisioning Inside a Secure Environment as Docker Containers Using Stroll File-System,conferencePaper,,IEEE,2016
12,P7591,Using Docker for automatic Galaxy deployment,conferencePaper,,,2016
13,P7627,Utilisation de Docker en bioinformatique dans le cloud de IFB,conferencePaper,,,2016
14,P102,A CyberGIS-Jupyter Framework for Geospatial Analytics at Scale,conferencePaper,"The interdisciplinary field of cyberGIS (geographic information science and systems (GIS) based on advanced cyberinfrastructure) has a major focus on data- and computation-intensive geospatial analytics. The rapidly growing needs across many application and science domains for such analytics based on disparate geospatial big data poses significant challenges to conventional GIS approaches. This paper describes CyberGIS-Jupyter, an innovative cyberGIS framework for achieving data-intensive, reproducible, and scalable geospatial analytics using the Jupyter Notebook based on ROGER - the first cyberGIS supercomputer. The framework adapts the Notebook with built-in cyberGIS capabilities to accelerate gateway application development and sharing while associated data, analytics and workflow runtime environments are encapsulated into application packages that can be elastically reproduced through cloud computing approaches. As a desirable outcome, data-intensive and scalable geospatial analytics can be efficiently developed and improved, and seamlessly reproduced among multidisciplinary users in a novel cyberGIS science gateway environment.",ACM,2017
15,P580,An Architecture for Genomics Analysis in a Clinical Setting Using Galaxy and Docker,journalArticle,"BackgroundNext Generation Sequencing is used on a daily basis to perform molecular analysis to determine subtypes of disease (e.g., in cancer) and to assist in the selection of the optimal treatment. Clinical bioinformatics handles the manipulation of the data generated by the sequencer, from the generation to the analysis and interpretation. Reproducibility and traceability are crucial issues in a clinical setting.ResultsWe have designed an approach based on the Docker container technology and Galaxy, the popular bioinformatics analysis support open-source software. Our solution simplifies the deployment of a small size analytical platform, and simplifies the process for the clinician. From the technical point of view, the tools embedded in the platform are isolated and versioned through docker images. Along the Galaxy platform, we also introduce the AnalysisManager, a solution allowing single-click analysis for the biologists and leveraging standardized bioinformatics APIs. We added a Shiny/R interactive environment to ease the visualization of the outputs.ConclusionsThe platform relies on containers and ensures the data traceability by recording analytical actions, and by associating inputs and outputs of the tools to the EDAM ontology through ReGaTe. The source code is freely available on Github at https://github.com/CARPEM/GalaxyDocker.",GigaScience,2017
16,P750,Apollo: collaborative and scalable manual genome annotation,conferencePaper,"Manual refinement of automated gene predictions using experimental evidence is a crucial step for improving the quality of a genome's annotation. Apollo, which utilizes the JBrowse genome browser, is a web-based genome annotation editor used by well over one hundred annotation projects. Annotation changes are reflected in real-time (like Google Docs), which facilitates distributed curation efforts. A single Apollo server can scale to support multiple genome projects and regulate access to multiple curators via fine-grained permissions. Apollo has been successfully integrated with Galaxy via Docker, and externally via its web-services, allowing the community to refine predicted genome elements generated via Galaxy workflows. Annotated genomic elements may be exported as FASTA, GFF3, or as a Chado database. We introduce two important features nearing completion. The first is variant annotation, which provides both a way to annotate and visualize variants as well as to visualize individual and combined effects of each variant on a given annotation. The second is coordinate transformation, which allows the visualization of two or more genomic regions, from the length of entire chromosomes to just a few exons, within an artificially constructed ” assemblage”. This facilitates annotation of genomic features split across two or more regions of a fragmented assembly, while informing potential improvements to the genome assembly in the process. Additionally, inter- and intragenic regions can be hidden to focus on regions of interest. For example, bringing the sequences of exons separated by thousands of base-pairs to be shown adjacently. Learn more at http://genomearchitect.org/.",,2017
17,P969,BGDMdocker: a Docker workflow for data mining and visualization of bacterial pan-genomes and biosynthetic gene clusters,journalArticle,"Recently, Docker technology has received increasing attention throughout the bioinformatics community. However, its implementation has not yet been mastered by most biologists, and thus its application in biological research has been limited. In order to popularize this technology in the field of bioinformatics and promote the use of publicly available bioinformatics tools, such as Dockerfiles and Images from communities, governmental, and private owners in Docker Hub Registry and other Docker-based resources, we introduce here a complete and accurate bioinformatics workflow based on Docker to analyze and visualize pan-genomes and biosynthetic gene clusters of bacteria. This provides a new solution for bioinformatics mining of big data from various public biological databases. This step-by-step guide creates an integrative workflow through a Dockerfile to allow researchers to build their own Image and run Container easily.",PeerJ Preprints,2017
18,P1116,Biopipe: A Lightweight System Enabling Comparison of Bioinformatics Tools and Workflows,journalArticle,"Analyzing next generation sequencing data always requires researchers to install many tools, prepare input data compliant to the required data format, and execute the tools in specific orders. Such tool installation and workflow execution process is tedious and error-prone, and becomes very challenging when researchers need to compare multiple alternative tool chains. To mitigate this problem, we developed a new lightweight and portable system, Biopipe, to simplify the creation and execution of bioinformatics tools and workflows, and to further enable the comparison between alternative tools or workflows. Biopipe allows users to create and edit workflows with user-friendly web interfaces, and automates tool installation as well as workflow synthesis by downloading and executing predefined Docker images. With Biopipe, biologists can easily experiment with and compare different bioinformatics tools and workflows without much computer science knowledge. There are mainly two parts in Biopipe: a web application and a standalone Java application. They are freely available at http://bench.cs.vt.edu:8282/Biopipe-Workflow-Editor-0.0.1/index.xhtml and https://code.vt.edu/saima5/Biopipe-Run-Workflow.",bioRxiv,2017
19,P1170,Bringing numerous methods for expression and promoter analysis to a public cloud computing service,journalArticle,"SummaryEvery year, a large number of novel algorithms are introduced to the scientific community for a myriad of applications, but using these across different research groups is often troublesome, due to suboptimal implementations and specific dependency requirements. This does not have to be the case, as public cloud computing services can easily house tractable implementations within self-contained dependency environments, making the methods easily accessible to a wider public. We have taken 14 popular methods, the majority related to expression data or promoter analysis, developed these up to a good implementation standard and housed the tools in isolated Docker containers which we integrated into the CyVerse Discovery Environment, making these easily usable for a wide community as part of the CyVerse UK project.Availability and implementationThe integrated apps can be found at http://www.cyverse.org/discovery-environment, while the raw code is available at https://github.com/cyversewarwick and the corresponding Docker images are housed at https://hub.docker.com/r/cyversewarwick/.Contactinfo@cyverse.warwick.ac.uk or D.L.Wild@warwick.ac.ukSupplementary informationSupplementary data are available at Bioinformatics online.",Bioinformatics,2017
20,P1184,"Building an open, collaborative, online infrastructure for bioinformatics training",conferencePaper,"With the advent of high-throughput platforms, life science data analysis is tightly linked to the use of bioinformatics tools, resources, and high-performance computing. However, the scientists who generate the data often do not have the knowledge required to be fully conversant with such analyses. To involve them in their own data analysis, these scientists must acquire bioinformatics vocabulary and skills through training. Data analysis training is particularly challenging without a computational background. The Galaxy framework is addressing this problem by offering a web-based, intuitive and accessible user interface to numerous bioinformatics tools. Recently, the Galaxy Training Network (GTN) set up a new open, collaborative, online model for delivering high-quality bioinformatics training material: http://galaxyproject.github.io/training-material. Each of the current 12 topics provides tutorials with hands-on, slides and interactive tours. Tours are a new way to go through an entire analysis, step by step inside Galaxy in an interactive and explorative way. All material is openly reviewed, and iteratively developed in one central repository by 40 contributors. Content is written in Markdown and, similarly to Software/Data Carpentry, the model separates presentation from content. In addition, the technological infrastructure needed to teach each topic is described with a list of needed tools. The data (citable via DOI) required for the hands-on, time and resource estimations and flavored Galaxy Docker images are also provided. This material is automatically propagated to Elixir's TeSS portal. With this community effort, the GTN offers an open, collaborative, FAIR and up-to-date infrastructure for delivering high-quality bioinformatics training for scientists.",,2017
21,P1425,"Chiminey: Connecting Scientists to HPC, Cloud and Big Data",journalArticle,"The enabling of scientific experiments increasingly includes data, software, computational and simulation elements, often embarrassingly parallel, long running and data-intensive. Frequently, such experiments are run in a cloud environment or on high-end clusters and supercomputers. Many disciplines in sciences and engineering (and outside computer science) find the requisite computational skills attractive on the one hand but distracting from their science domain on the other. We developed Chiminey under directions by quantum physicists and molecular biologists, to ease the steep learning curve in data management and software platforms, required for the complex computational target systems. Chiminey is a smart connector mediating running specialist algorithms developed for workstations with moderately large data set and relatively small computational grunt. This connector allows the domain scientists to choose the target platform and then manages it automatically; it accepts all the necessary parameters to run many instances of their program regardless of whether this runs on a peak supercomputer, a commercial cloud like Amazon EC2 or (in Australia) the national federated university cloud system NeCTAR. Chiminey negotiates with target system schedulers, dashboards and data bases and provides an easy-to-use dashboard interface to the running jobs, regardless of the specific target platform. The smart connector encapsulates and virtualises a number of further aspects that the domain scientists directing our effort found necessary or desirable. In this article we present Chiminey and guide the reader through a hands-on tutorial of this open-source platform. The only requirement is that the reader has access to one of the supported clouds or cluster platforms – and very likely there is a matching one. The tutorial stages range in difficulty from requiring no to little technical background through to advanced sections, such as programming your own domain-specific extension on top of Chiminey application programmer interfaces. The different exercises we demonstrate include: installing the Docker deployment environment and Chiminey system; registering resources for file stores, Hadoop MapReduce and cloud virtual machines; activating hrmclite and wordcount smart connectors – two demonstrators; running a smart connector and investigating the resulting output files; and building a new smart connector. We also discuss briefly where to find more detailed information on, and what is involved in, contributing to the Chiminey open source code base.",Big Data Research,2017
22,P1587,Coherent Application Delivery on Hybrid Distributed Computing Infrastructures of Virtual Machines and Docker Containers,conferencePaper,,IEEE,2017
23,P1928,Container-based Virtual Elastic Clusters,journalArticle,"EC4Docker create virtual clusters made of containers instead of virtual machines EC4Docker creates self managed elastic clusters that adapt its size to the workload The elasticity is enhanced because containers boot faster than virtual machines Using tools like Docker swarm enables to span the containers across a set of hosts eScience demands large-scale computing clusters to support the efficient execution of resource-intensive scientific applications. Virtual Machines (VMs) have introduced the ability to provide customizable execution environments, at the expense of performance loss for applications. However, in recent years, containers have emerged as a light-weight virtualization technology compared to VMs. Indeed, the usage of containers for virtual clusters allows better performance for the applications and fast deployment of additional working nodes, for enhanced elasticity. This paper focuses on the deployment, configuration and management of Virtual Elastic computer Clusters (VEC) dedicated to processs scientific workloads. The nodes of the scientific cluster are hosted in containers running on bare-metal machines. The open-source tool Elastic Cluster for Docker (EC4Docker) is introduced, integrated with Docker Swarm to create auto-scaled virtual computer clusters of containers across distributed deployments. We also discuss the benefits and limitations of this solution and analyse the performance of the developed tools under a real scenario by means of a scientific use case that demonstrates the feasibility of the proposed approach.",Journal of Systems and Software,2017
24,P1930,Containerizing Conda: Experiences Building 2000 Applications As Portable Docker And Singularity Images,conferencePaper,,,2017
25,P2571,"Dugong: a Docker image, based on Ubuntu Linux, focused on reproducibility and replicability for bioinformatics analyses",journalArticle,"Summary: This manuscript introduces and describes Dugong, a Docker image based on Ubuntu 16.04, which automates installation of more than 3500 bioinformatics tools (along with their respective libraries and dependencies), in alternative computational environments. The software operates through a user-friendly XFCE4 graphic interface that allows software management and installation by users not fully familiarized with the Linux command line and provides the Jupyter Notebook to assist in the delivery and exchange of consistent and reproducible protocols and results across laboratories, assisting in the development of open science projects.Availability and implementation: Source code and instructions for local installation are available at https://github.com/DugongBioinformatics, under the MIT open source license.Contact:Luiz.Nunes@ufabc.edu.brSupplementary Information: Supplementary data available at: https://dugongbioinformatics.github.io/",Bioinformatics,2017
26,P2786,Enhancing Knowledge Discovery from Cancer Genomics Data with Galaxy.,journalArticle,"The field of cancer genomics has demonstrated the power of massively parallel sequencing techniques to inform on the genes and specific alterations that drive tumor onset and progression. Although large comprehensive sequence data sets continue to be made increasingly available, data analysis remains an ongoing challenge, particularly for laboratories lacking dedicated resources and bioinformatics expertise. To address this, we have produced a collection of Galaxy tools that represent many popular algorithms for detecting somatic genetic alterations from cancer genome and exome data. We developed new methods for parallelization of these tools within Galaxy to accelerate runtime and have demonstrated their usability and summarized their runtimes on multiple cloud service providers. Some tools represent extensions or refinement of existing toolkits to yield visualizations suited to cohort-wide cancer genomic analysis. For example, we present Oncocircos and Oncoprintplus, which generate data-rich summaries of exome-derived somatic mutation. Workflows that integrate these to achieve data integration and visualizations are demonstrated on a cohort of 96 diffuse large B-cell lymphomas and enabled the discovery of multiple candidate lymphoma-related genes. Our toolkit is available from our GitHub repository as Galaxy tool and dependency definitions and has been deployed using virtualization on multiple platforms including Docker. \\copyright The Authors 2017. Published by Oxford University Press.",GigaScience,2017
27,P2951,Experiences Building 2000 Applications As Portable Docker And Singularity Images,conferencePaper,"Over the last 5 years, container technologies have gone from a novelty, to a niche, to a necessity for many research communities. As both infrastructure and cyberinfrastructure providers, the authors have worked to support their user communities in their adoption and use of these technologies. While Docker [1] is still the dominant mainstream container technology, the space has begun to fragment as containers move past the hype and into broad usage. Alternative image formats and container runtimes have emerged, providing compelling use cases and advantages over Docker for compute containers [2][3][4][5].",,2017
28,P3330,GeNNet: an integrated platform for unifying scientific workflows and graph databases for transcriptome data analysis,journalArticle,"There are many steps in analyzing transcriptome data, from the acquisition of raw data to the selection of a subset of representative genes that explain a scientific hypothesis. The data produced can be represented as networks of interactions among genes and these may additionally be integrated with other biological databases, such as Protein-Protein Interactions, transcription factors and gene annotation. However, the results of these analyses remain fragmented, imposing difficulties, either for posterior inspection of results, or for meta-analysis by the incorporation of new related data. Integrating databases and tools into scientific workflows, orchestrating their execution, and managing the resulting data and its respective metadata are challenging tasks. Additionally, a great amount of effort is equally required to run in-silico experiments to structure and compose the information as needed for analysis. Different programs may need to be applied and different files are produced during the experiment cycle. In this context, the availability of a platform supporting experiment execution is paramount. We present GeNNet, an integrated transcriptome analysis platform that unifies scientific workflows with graph databases for selecting relevant genes according to the evaluated biological systems. It includes GeNNet-Wf, a scientific workflow that pre-loads biological data, pre-processes raw microarray data and conducts a series of analyses including normalization, differential expression inference, clusterization and gene set enrichment analysis. A user-friendly web interface, GeNNet-Web, allows for setting parameters, executing, and visualizing the results of GeNNet-Wf executions. To demonstrate the features of GeNNet, we performed case studies with data retrieved from GEO, particularly using a single-factor experiment in different analysis scenarios. As a result, we obtained differentially expressed genes for which biological functions were analyzed. The results are integrated into GeNNet-DB, a database about genes, clusters, experiments and their properties and relationships. The resulting graph database is explored with queries that demonstrate the expressiveness of this data model for reasoning about gene interaction networks. GeNNet is the first platform to integrate the analytical process of transcriptome data with graph databases. It provides a comprehensive set of tools that would otherwise be challenging for non-expert users to install and use. Developers can add new functionality to components of GeNNet. The derived data allows for testing previous hypotheses about an experiment and exploring new ones through the interactive graph database environment. It enables the analysis of different data on humans, rhesus, mice and rat coming from Affymetrix platforms. GeNNet is available as an open source platform at https://github.com/raquele/GeNNet and can be retrieved as a software container with the command docker pull quelopes/gennet.",PeerJ,2017
29,P4330,Jupyter and Galaxy: Easing entry barriers into complex data analyses for biomedical researchers,journalArticle,"What does it take to convert a heap of sequencing data into a publishable result? First, common tools are employed to reduce primary data (sequencing reads) to a form suitable for further analyses (i.e., the list of variable sites). The subsequent exploratory stage is much more ad hoc and requires the development of custom scripts and pipelines, making it problematic for biomedical researchers. Here, we describe a hybrid platform combining common analysis pathways with the ability to explore data interactively. It aims to fully encompass and simplify the \""raw data-to-publication\"" pathway and make it reproducible. Galaxy users can utilize a large number of tools and workflows. What they could not previously do is run ad hoc scripts and arbitrary tools within their Galaxy instance. This was very limiting, as initial analyses of data often involve interactive exploration with tools like Jupyter or RStudio—powerful platforms that are becoming increasingly popular in life sciences. Here, we showcase Galaxy Interactive Environment framework, designed to combine Galaxy's tools and workflows with environments such as Jupyter.",PLOS Computational Biology,2017
30,P4509,Making Galaxy user Interface Pluggable with Webhooks,conferencePaper,"Historically Galaxy's implementation of the user interface (UI) did not allow for rich user experience features familiar for users of Facebook, Gmail, GitHub and other sites. To address this Galaxy has been undergoing an architectural transformation towards API-driven framework design. Here we describe benefits of this shift – the new design philosophy allows us to dramatically improve user experience by enabling features and uses that were simply not possible before. In particular, the addition of Galaxy Webhooks – a plug-in system designed for customization of individual Galaxy instances – is one of the prominent developments enabled by the new architecture. Webhooks are configurable and often community-contributed pieces of code altering the UI and providing additional features. The primary benefit to the community will be the ability to personalize Galaxy instances tailoring them to the needs of individual groups. Notable examples of webhooks: Tool Describing Tours - providing capability for executing a 'demo' run of any tool using the tool's own test case and data. This is the ideal medium for guiding and educating users by directly exposing them to tools parameters, inputs and outputs. Overlay Search - allowing rich exploration across all objects in the Galaxy including datasets, tools, histories, workflows, libraries and so on. Tool Flavoring - generating a list of installed tools that can be readily packaged into docker images mimicking the original Galaxy's toolset. We believe webhooks represent a logical result of project's sustained focus on building robust, reliable framework for integration of tools and plugins.",,2017
31,P5153,NGLview–interactive molecular graphics for Jupyter notebooks,journalArticle,"SummaryNGLview is a Jupyter/IPython widget to interactively view molecular structures as well as trajectories from molecular dynamics simulations. Fast and scalable molecular graphics are provided through the NGL Viewer. The widget supports showing data from the file-system, online data bases and from objects of many popular analysis libraries including mdanalysis, mdtraj, pytraj, rdkit and more.Availability and implementationThe source code is freely available under the MIT license at https://github.com/arose/nglview. Python packages are available from PyPI and bioconda. NGLview uses Python on the server-side and JavaScript on the client. The integration with Jupyter is done through the ipywidgets package. The NGL Viewer is embedded client-side to provide WebGL accelerated molecular graphics. Contactasr.moin@gmail.com",Bioinformatics,2017
32,P5388,P-MartCancer–Interactive Online Software to Enable Analysis of Shotgun Cancer Proteomic Datasets,journalArticle,"P-MartCancer is an interactive web-based software environment that enables statistical analyses of peptide or protein data, quantitated from mass spectrometry–based global proteomics experiments, without requiring in-depth knowledge of statistical programming. P-MartCancer offers a series of statistical modules associated with quality assessment, peptide and protein statistics, protein quantification, and exploratory data analyses driven by the user via customized workflows and interactive visualization. Currently, P-MartCancer offers access and the capability to analyze multiple cancer proteomic datasets generated through the Clinical Proteomics Tumor Analysis Consortium at the peptide, gene, and protein levels. P-MartCancer is deployed as a web service (https://pmart.labworks.org/cptac.html), alternatively available via Docker Hub (https://hub.docker.com/r/pnnl/pmart-web/). Cancer Res; 77(21); e47–50. ©2017 AACR.",Cancer Research,2017
33,P5885,Recent object formation in the core of Galaxy,conferencePaper,"Galaxy pursues accessibility, transparency and reproducibility in data-intensive science. It is a web-based framework that automatically tracks analyses and enables researchers to use advanced computational tools and resources while focusing on the research questions rather than the supporting compute infrastructure. In this report we highlight recently added and enhanced features and discuss future directions. The most prominent recent developments are: Conda for dependency resolution. Galaxy has deprecated its own solution (Tool Shed package recipes) for the software package management and embraced the Conda manager for these purposes. This makes tool dependencies more reliable and stable, and they are also easier to test and faster to develop. Interactive Environments integrated into Galaxy interface are powerful tools for interactive ad hoc analyses as demonstrated in a recent paper (doi.org/10.1101/075457). In addition to the Jupyter notebook and RStudio, the Galaxy community has expanded the number of available IEs to include Phinch for metagenomic data visualization, Ethercalc for tabular and csv data, and Neo4j for graph database manipulation. Galaxy Webhooks are a system for adding plugins allowing for customization of individual instances. Webhooks are admin-enabled and often community-contributed pieces of code that alter the interface and offer extra features. Popular webhooks will likely become part of the core UI in the future. Dataset Collections are becoming increasingly powerful - you can now directly create them when uploading datasets, flatten and import them to data libraries, and use many new and improved collection operations tools. Various toolkits were enhanced to handle collections natively. Other notable advancements: You can start up an independent chat server and connect it to Galaxy enabling users to share and collaborate without leaving the analysis interface. Galaxy also supports compressed FASTQ formats allowing to save storage and remove unnecessary steps from workflows. Tool cache and 'hot reload' functionalities have also been added, enabling administrators to update tools without a server restart. The tool cache has also made Galaxy startup much faster, especially for instances with many tools. Histories now allow for drag&drop of datasets as tool inputs and also can propagate dataset tags through tool executions. In the last year 3 new IUC members joined our ranks totalling 15 now. They handled 401 pull requests in last 12 months and added numerous contributions to Bioconda and other connected projects.",,2017
34,P5981,Reproducibility and Practical Adoption of GEOBIA with Open-Source Software in Docker Containers,journalArticle,"Geographic Object-Based Image Analysis (GEOBIA) mostly uses proprietary software,but the interest in Free and Open-Source Software (FOSS) for GEOBIA is growing. This interest stems not only from cost savings, but also from benefits concerning reproducibility and collaboration. Technical challenges hamper practical reproducibility, especially when multiple software packages are required to conduct an analysis. In this study, we use containerization to package a GEOBIA workflow in a well-defined FOSS environment. We explore the approach using two software stacks to perform an exemplary analysis detecting destruction of buildings in bi-temporal images of a conflict area. The analysis combines feature extraction techniques with segmentation and object-based analysis to detect changes using automatically-defined local reference values and to distinguish disappeared buildings from non-target structures. The resulting workflow is published as FOSS comprising both the model and data in a ready to use Docker image and a user interface for interaction with the containerized workflow. The presented solution advances GEOBIA in the following aspects: higher transparency of methodology; easier reuse and adaption of workflows; better transferability between operating systems; complete description of the software environment; and easy application of workflows by image analysis experts and non-experts. As a result, it promotes not only the reproducibility of GEOBIA, but also its practical adoption.",Remote Sensing,2017
35,P6247,Scientific Notebook Software: Applications for Academic Radiology,journalArticle,"The goal of this paper is to introduce the concept of scientific notebook software, and to illustrate how it can be used to document a research project, to perform image analysis and to create interactive teaching tools. We describe the installation of scientific notebook software known as Project Jupyter, which is free, open-source and available for the Macintosh, Windows and Linux operating systems. We have created two scientific notebooks that demonstrate applications germane to radiologists, particularly those in academic radiology. The first notebook provides a tutorial that summarizes basic features of the Project Jupyter notebook, and gives numerous examples of how the notebooks can display explicatory text, perform statistical computations, and display plots, interactive graphics, and audio files. The second notebook provides a toolbox for viewing and manipulating images in the Digital Imaging and Communications in Medicine (DICOM) format. Scientific notebook software allows its users to document their work in a form that combines text, graphics, images, data, interactive calculations and image analysis within a single document. Scientific notebooks also provide interactive teaching tools, which are can help explain complex topics in imaging physics to residents.",Current Problems in Diagnostic Radiology,2017
36,P6322,SeqBox: RNAseq/ChIPseq reproducible analysis on a consumer game computer,journalArticle,"SummaryShort reads sequencing technology has been used for more than a decade now. However, the analysis of RNAseq and ChIPseq data is still computational demanding and the simple access to raw data does not guarantee results reproducibility between laboratories. To address these two aspects, we developed SeqBox, a cheap, efficient and reproducible RNAseq/ChIPseq hardware/software solution based on NUC6I7KYK mini-PC (an Intel consumer game computer with a fast processor and a high performance SSD disk), and Docker container platform. In SeqBox the analysis of RNAseq and ChIPseq data is supported by a friendly GUI. This allows access to fast and reproducible analysis also to scientists with/without scripting experience.Availability and ImplementationDocker container images, docker4seq package and the GUI are available at http://www.bioinformatica.unito.it/reproducibile.bioinformatics.html.Contactbeccuti@di.unito.itSupplementary informationSupplementary data are available at Bioinformatics online.",Bioinformatics,2017
37,P6474,SNVPhyl: a single nucleotide variant phylogenomics pipeline for microbial genomic epidemiology,journalArticle,"The recent widespread application of whole-genome sequencing (WGS) for microbial disease investigations has spurred the development of new bioinformatics tools, including a notable proliferation of phylogenomics pipelines designed for infectious disease surveillance and outbreak investigation. Transitioning the use of WGS data out of the research laboratory and into the front lines of surveillance and outbreak response requires user-friendly, reproducible and scalable pipelines that have been well validated. Single Nucleotide Variant Phylogenomics (SNVPhyl) is a bioinformatics pipeline for identifying high-quality single-nucleotide variants (SNVs) and constructing a whole-genome phylogeny from a collection of WGS reads and a reference genome. Individual pipeline components are integrated into the Galaxy bioinformatics framework, enabling data analysis in a user-friendly, reproducible and scalable environment. We show that SNVPhyl can detect SNVs with high sensitivity and specificity, and identify and remove regions of high SNV density (indicative of recombination). SNVPhyl is able to correctly distinguish outbreak from non-outbreak isolates across a range of variant-calling settings, sequencing-coverage thresholds or in the presence of contamination. SNVPhyl is available as a Galaxy workflow, Docker and virtual machine images, and a Unix-based command-line application. SNVPhyl is released under the Apache 2.0 license and available at http://snvphyl.readthedocs.io/ or at https://github.com/phac-nml/snvphyl-galaxy.",Microbial Genomics,2017
38,P6914,"The Dockstore: enabling modular, community-focused sharing of Docker-based genomics tools and workflows",journalArticle,,F1000Research,2017
39,P7590,Using Docker Compose for the Simple Deployment of an Integrated Drug Target Screening Platform.,journalArticle,"Docker virtualization allows for software tools to be executed in an isolated and controlled environment referred to as a container. In Docker containers, dependencies are provided exactly as intended by the developer and, consequently, they simplify the distribution of scientific software and foster reproducible research. The Docker paradigm is that each container encapsulates one particular software tool. However, to analyze complex biomedical data sets, it is often necessary to combine several software tools into elaborate workflows. To address this challenge, several Docker containers need to be instantiated and properly integrated, which complicates the software deployment process unnecessarily. Here, we demonstrate how an extension to Docker, Docker compose, can be used to mitigate these problems by providing a unified setup routine that deploys several tools in an integrated fashion. We demonstrate the power of this approach by example of a Docker compose setup for a drug target screening platform consisting of five integrated web applications and shared infrastructure, deployable in just two lines of codes.",Journal of integrative bioinformatics,2017
40,P7752,wft4galaxy: a workflow testing tool for galaxy,journalArticle,"Motivation: Workflow managers for scientific analysis provide a high-level programming platform facilitating standardization, automation, collaboration and access to sophisticated computing resources. The Galaxy workflow manager provides a prime example of this type of platform. As compositions of simpler tools, workflows effectively comprise specialized computer programs implementing often very complex analysis procedures. To date, no simple way to automatically test Galaxy workflows and ensure their correctness has appeared in the literature. Results: With wft4galaxy we offer a tool to bring automated testing to Galaxy workflows, making it feasible to bring continuous integration to their development and ensuring that defects are detected promptly. wft4galaxy can be easily installed as a regular Python program or launched directly as a Docker container—the latter reducing installation effort to a minimum. Availability and implementation: Available at https://github.com/phnmnl/wft4galaxy under the Academic Free License v3.0.",Bioinformatics,2017
41,P424,Accumulating computational resource usage of genomic data analysis workflow to optimize cloud computing instance selection,journalArticle,"Background: Container virtualization technologies such as Docker became popular in the bioinformatics domain as they improve portability and reproducibility of software deployment. Along with software packaged in containers, the workflow description standards Common Workflow Language also enabled to perform data analysis on multiple different computing environments with ease. These technologies accelerate the use of on-demand cloud computing platform which can scale out according to the amount of data. However, to optimize the time and the budget on a use of cloud, users need to select a suitable instance type corresponding to the resource requirements of their workflows. Results: We developed CWL-metrics, a system to collect runtime metrics of Docker containers and workflow metadata to analyze resource requirement of workflows. We demonstrated the analysis by using seven transcriptome quantification workflows on six instance types. The result showed instance type options of lower financial cost and faster execution time with required amount of computational resources. Conclusions: The summary of resource requirements of workflow executions provided by CWL-metrics can help users to optimize the selection of cloud computing instance. The runtime metrics data also accelerate to share workflows among different workflow management frameworks.",bioRxiv,2018
42,P590,An Evaluation of Software-Based Traffic Generators using Docker,thesis,DiVA portal is a finding tool for research publications and student theses written at the following 47 universities and research institutions.,,2018
43,P1090,BioInstaller: a comprehensive R package to construct interactive and reproducible biological data analysis applications based on the R platform,journalArticle,"The increase in bioinformatics resources such as tools/scripts and databases poses a great challenge for users seeking to construct interactive and reproducible biological data analysis applications. Here, we propose an open-source, comprehensive, flexible R package named BioInstaller that consists of the R functions, Shiny application, the HTTP representational state transfer application programming interfaces, and a docker image. BioInstaller can be used to collect, manage and share various types of bioinformatics resources and perform interactive and reproducible data analyses based on the extendible Shiny application with Tom’s Obvious, Minimal Language and SQLite format databases. The source code of BioInstaller is freely available at our lab website, http://bioinfo.rjh.com.cn/labs/jhuang/tools/bioinstaller, the popular package host GitHub, https://github.com/JhuangLab/BioInstaller, and the Comprehensive R Archive Network, https://CRAN.R-project.org/package=BioInstaller. In addition, a docker image can be downloaded from DockerHub (https://hub.docker.com/r/bioinstaller/bioinstaller).",PeerJ,2018
44,P1091,BioInstaller: a comprehensive R package to integrate bioinformatics resources,journalArticle,"The number of bioinformatics resources, such as tools/scripts and databases are growing exponentially. This poses a great challenge for users to access, manage, and integrate the corresponding bioinformatics resources. To facilitate the request, we proposed a comprehensive R package, BioInstaller, which includes the R functions, Shiny application, and the HTTP representational state transfer (REST) application programming interfaces (APIs). We also established a community-based configuration pool to collect, access and share bioinformatics resources. The source code of BioInstaller is freely available at our lab website\n            \n            or popular package host GitHub at:\n            \n            . Also, a docker image can be downloaded from DockerHub (https://hub.docker.com/r/bioinstaller).",PeerJ Preprints,2018
45,P1094,BioJupies: Automated Generation of Interactive Notebooks for RNA-Seq Data Analysis in the Cloud,journalArticle,"Summary\nBioJupies is a web application that enables the automated creation, storage, and deployment of Jupyter Notebooks containing RNA-seq data analyses. Through an intuitive interface, novice users can rapidly generate tailored reports to analyze and visualize their own raw sequencing files, gene expression tables, or fetch data from >9,000 published studies containing >300,000 preprocessed RNA-seq samples. Generated notebooks have the executable code of the entire pipeline, rich narrative text, interactive data visualizations, differential expression, and enrichment analyses. The notebooks are permanently stored in the cloud and made available online through a persistent URL. The notebooks are downloadable, customizable, and can run within a Docker container. By providing an intuitive user interface for notebook generation for RNA-seq data analysis, starting from the raw reads all the way to a complete interactive and reproducible report, BioJupies is a useful resource for experimental and computational biologists. BioJupies is freely available as a web-based application from http://biojupies.cloud.",Cell Systems,2018
46,P1188,Building containerized workflows using the BioDepot-workflow-builder (Bwb),journalArticle,"We present the BioDepot-workflow-builder (Bwb), a software tool that allows users to create and execute reproducible bioinformatics workflows using a drag-and-drop interface. Graphical widgets represent Docker containers executing a modular task. Widgets are then graphically linked to build bioinformatics workflows that can be reproducibly deployed across different local and cloud platforms. Each widget contains a form-based user interface to facilitate parameter entry and a console to display intermediate results. Bwb provides tools for rapid customization of widgets, containers and workflows. Saved workflows can be shared using Bwb9s native format or exported as shell scripts.",bioRxiv,2018
47,P2043,CyberGIS-Jupyter for reproducible and scalable geospatial analytics,journalArticle,"The interdisciplinary field of cyberGIS (geographic information science and systems (GIS) based on advanced cyberinfrastructure) has a major focus on data- and computation-intensive geospatial analytics. The rapidly growing needs across many application and science domains for such analytics based on disparate geospatial big data poses significant challenges to conventional GIS approaches. This paper describes CyberGIS-Jupyter, an innovative cyberGIS framework for achieving data-intensive, reproducible, and scalable geospatial analytics using Jupyter Notebook based on ROGER, the first cyberGIS supercomputer. The framework adapts the Notebook with built-in cyberGIS capabilities to accelerate gateway application development and sharing while associated data, analytics, and workflow runtime environments are encapsulated into application packages that can be elastically reproduced through cloud-computing approaches. As a desirable outcome, data-intensive and scalable geospatial analytics can be efficiently developed and improved and seamlessly reproduced among multidisciplinary users in a novel cyberGIS science gateway environment.",Concurrency and Computation: Practice and Experience,2018
48,P2488,DNAp: A Pipeline for DNA-seq Data Analysis,journalArticle,"Next-generation sequencing is empowering genetic disease research. However, it also brings significant challenges for efficient and effective sequencing data analysis. We built a pipeline, called DNAp, for analyzing whole exome sequencing (WES) and whole genome sequencing (WGS) data, to detect mutations from disease samples. The pipeline is containerized, convenient to use and can run under any system, since it is a fully automatic process in Docker container form. It is also open, and can be easily customized with user intervention points, such as for updating reference files and different software or versions. The pipeline has been tested with both human and mouse sequencing datasets, and it has generated mutations results, comparable to published results from these datasets, and reproducible across heterogeneous hardware platforms. The pipeline DNAp, funded by the US Food and Drug Administration (FDA), was developed for analyzing DNA sequencing data of FDA. Here we make DNAp an open source, with the software and documentation available to the public at \n                  http://bioinformatics.astate.edu/dna-pipeline/\n                  \n                .",Scientific Reports,2018
49,P2500,DockerBIO: web application for efficient use of bioinformatics Docker images,journalArticle,"Background and Objective Docker is a light containerization program that shows almost the same performance as a local environment. Recently, many bioinformatics tools have been distributed as Docker images that include complex settings such as libraries, configurations, and data if needed, as well as the actual tools. Users can simply download and run them without making the effort to compile and configure them, and can obtain reproducible results. In spite of these advantages, several problems remain. First, there is a lack of clear standards for distribution of Docker images, and the Docker Hub often provides multiple images with the same objective but different uses. For these reasons, it can be difficult for users to learn how to select and use them. Second, Docker images are often not suitable as a component of a pipeline, because many of them include big data. Moreover, a group of users can have difficulties when sharing a pipeline composed of Docker images. Users of a group may modify scripts or use different versions of the data, which causes inconsistent results. Methods and Results To handle the problems described above, we developed a Java web application, DockerBIO, which provides reliable, verified, light-weight Docker images for various bioinformatics tools and for various kinds of reference data. With DockerBIO, users can easily build a pipeline with tools and data registered at DockerBIO, and if necessary, users can easily register new tools or data. Built pipelines are registered in DockerBIO, which provides an efficient running environment for the pipelines registered at DockerBIO. This enables user groups to run their pipelines without expending much effort to copy and modify them.",PeerJ,2018
50,P2978,Exploring Genomic Datasets: from Batch to Interactive and Back,conferencePaper,"Genomic data management is focused on achieving high performance over big datasets using batch, cloud-based architectures; this enables the execution of massive pipelines, but hampers the capability of exploring the solution space when it is not well-defined, by choosing different experimental samples or query extraction parameters. We present PyGMQL, a Python-based interoperability software layer that enables testing of experimental pipelines; PyGMQL solves the impedance mismatch between a batch execution environment and the agile programming style of Python, and provides transparency of access when exploration requires integrating local and remote resources. Wrapping PyGMQL and Python primitives within Jupyter notebooks guarantees reproducibility of the pipeline when used in different contexts or by different scientists. The software is freely available at https://github.com/DEIB-GECO/PyGMQL.",,2018
51,P4405,Leveraging OpenStack and Ceph for a Controlled-Access Data Cloud,conferencePaper,"While traditional HPC has and continues to satisfy most workflows, a new generation of researchers has emerged looking for sophisticated, scalable, on-demand, and self-service control of compute infrastructure in a cloud-like environment. Many also seek safe harbors to operate on or store sensitive and/or controlled-access data in a high capacity environment. To cater to these modern users, the Minnesota Supercomputing Institute designed and deployed Stratus, a locally-hosted cloud environment powered by the OpenStack platform, and backed by Ceph storage. The subscription-based service complements existing HPC systems by satisfying the following unmet needs of our users: a) on-demand availability of compute resources; b) long-running jobs (i.e., > 30 days); c) container-based computing with Docker; and d) adequate security controls to comply with controlled-access data requirements. This document provides an in-depth look at the design of Stratus with respect to security and compliance with the NIH's controlled-access data policy. Emphasis is placed on lessons learned while integrating OpenStack and Ceph features into a so-called \""walled garden\"", and how those technologies influenced the security design. Many features of Stratus, including tiered secure storage with the introduction of a controlled-access data \""cache\"", fault-tolerant live-migrations, and fully integrated two-factor authentication, depend on recent OpenStack and Ceph features.",ACM,2018
52,P5393,PaDuA: A Python Library for High-Throughput (Phospho)proteomics Data Analysis,journalArticle,"The increased speed and sensitivity in mass spectrometry-based proteomics has encouraged its use in biomedical research in recent years. Large-scale detection of proteins in cells, tissues, and whole organisms yields highly complex quantitative data, the analysis of which poses significant challenges. Standardized proteomic workflows are necessary to ensure automated, sharable, and reproducible proteomics analysis. Likewise, standardized data processing workflows are also essential for the overall reproducibility of results. To this purpose, we developed PaDuA, a Python package optimized for the processing and analysis of (phospho)proteomics data. PaDuA provides a collection of tools that can be used to build scripted workflows within Jupyter Notebooks to facilitate bioinformatics analysis by both end-users and developers.",Journal of Proteome Research,2018
53,P5612,PPIP: Automated Software for Identification of Bioactive Endogenous Peptides,journalArticle,"Endogenous peptides play an important role in multiple biological processes in many species. Liquid chromatography coupled to tandem mass spectrometry (LC–MS/MS) is an important technique for detecting these peptides on a large scale. We present PPIP, which is a dedicated peptidogenomics software for identifying endogenous peptides based on peptidomics and RNA-Seq data. This software automates the de novo transcript assembly based on RNA-Seq data, construction of a protein reference database based on the de novo assembled transcripts, peptide identification, function analysis, and HTML-based report generation. Different function components are integrated using Docker technology. The Docker image of PPIP is available at https://hub.docker.com/r/shawndp/ppip, and the source code under GPL-3 license is available at https://github.com/Shawn-Xu/PPIP. A user manual of PPIP is available at https://shawn-xu.github.io/PPIP.",Journal of Proteome Research,2018
54,P5823,QuanTP: A Software Resource for Quantitative Proteo-Transcriptomic Comparative Data Analysis and Informatics,journalArticle,"Next-generation sequencing technologies, coupled to advances in mass-spectrometry-based proteomics, have facilitated system-wide quantitative profiling of expressed mRNA transcripts and proteins. Proteo-transcriptomic analysis compares the relative abundance levels of transcripts and their corresponding proteins, illuminating discordant gene product responses to perturbations. These results reveal potential post-transcriptional regulation, providing researchers with important new insights into underlying biological and pathological disease mechanisms. To carry out proteo-transcriptomic analysis, researchers require software that statistically determines transcript–protein abundance correlation levels and provides results visualization and interpretation functionality, ideally within a flexible, user-friendly platform. As a solution, we have developed the QuanTP software within the Galaxy platform. The software offers a suite of tools and functionalities critical for proteo-transcriptomics, including statistical algorithms for assessing the correlation between single transcript–protein pairs as well as across two cohorts, outlier identification and clustering, along with a diverse set of results visualizations. It is compatible with analyses of results from single experiment data or from a two-cohort comparison of aggregated replicate experiments. The tool is available in the Galaxy Tool Shed through a cloud-based instance and a Docker container. In all, QuanTP provides an accessible and effective software resource, which should enable new multiomic discoveries from quantitative proteo-transcriptomic data sets.",Journal of Proteome Research,2018
55,P5914,Reference environments: A universal tool for reproducibility in computational biology,journalArticle,"The drive for reproducibility in the computational sciences has provoked discussion and effort across a broad range of perspectives: technological, legislative/policy, education, and publishing. Discussion on these topics is not new, but the need to adopt standards for reproducibility of claims made based on computational results is now clear to researchers, publishers and policymakers alike. Many technologies exist to support and promote reproduction of computational results: containerisation tools like Docker, literate programming approaches such as Sweave, knitr, iPython or cloud environments like Amazon Web Services. But these technologies are tied to specific programming languages (e.g. Sweave/knitr to R; iPython to Python) or to platforms (e.g. Docker for 64-bit Linux environments only). To date, no single approach is able to span the broad range of technologies and platforms represented in computational biology and biotechnology. To enable reproducibility across computational biology, we demonstrate an approach and provide a set of tools that is suitable for all computational work and is not tied to a particular programming language or platform. We present published examples from a series of papers in different areas of computational biology, spanning the major languages and technologies in the field (Python/R/MATLAB/Fortran/C/Java). Our approach produces a transparent and flexible process for replication and recomputation of results. Ultimately, its most valuable aspect is the decoupling of methods in computational biology from their implementation. Separating the 'how' (method) of a publication from the 'where' (implementation) promotes genuinely open science and benefits the scientific community as a whole.",arXiv,2018
56,P5998,Reproducible Research in Document Analysis and Recognition,bookSection,"With reproducible research becoming a de facto standard in computational sciences, many approaches have been explored to enable researchers in other disciplines to adopt this standard. In this paper, we explore the importance of reproducible research in the field of document analysis and recognition and in the Computer Science field as a whole. First, we report on the difficulties that one can face in trying to reproduce research in the current publication standards. These difficulties for a large percentage of research may include missing raw or original data, a lack of tidied up version of the data, no source code available, or lacking the software to run the experiment. Furthermore, even when we have all these tools available, we found it was not a trivial task to replicate the research due to lack of documentation and deprecated dependencies. In this paper, we offer a solution to these reproducible research issues by utilizing container technologies such as Docker. As an example, we revisit the installation and execution of OCRSpell which we reported on and implemented in 1994. While the code for OCRSpell is freely available on github, we continuously get emails from individuals who have difficulties compiling and using it in modern hardware platforms. We walk through the development of an OCRSpell Docker container for creating an image, uploading such an image, and enabling others to easily run this program by simply downloading the image and running the container.","Springer, Cham",2018
57,P6774,Tellurium notebooks—An environment for reproducible dynamical modeling in systems biology,journalArticle,"The considerable difficulty encountered in reproducing the results of published dynamical models limits validation, exploration and reuse of this increasingly large biomedical research resource. To address this problem, we have developed Tellurium Notebook, a software system for model authoring, simulation, and teaching that facilitates building reproducible dynamical models and reusing models by 1) providing a notebook environment which allows models, Python code, and narrative to be intermixed, 2) supporting the COMBINE archive format during model development for capturing model information in an exchangeable format and 3) enabling users to easily simulate and edit public COMBINE-compliant models from public repositories to facilitate studying model dynamics, variants and test cases. Tellurium Notebook, a Python–based Jupyter–like environment, is designed to seamlessly inter-operate with these community standards by automating conversion between COMBINE standards formulations and corresponding in–line, human–readable representations. Thus, Tellurium brings to systems biology the strategy used by other literate notebook systems such as Mathematica. These capabilities allow users to edit every aspect of the standards–compliant models and simulations, run the simulations in–line, and re–export to standard formats. We provide several use cases illustrating the advantages of our approach and how it allows development and reuse of models without requiring technical knowledge of standards. Adoption of Tellurium should accelerate model development, reproducibility and reuse.",PLOS Computational Biology,2018
58,P6800,Ten Simple Rules for Reproducible Research in Jupyter Notebooks,journalArticle,"Reproducibility of computational studies is a hallmark of scientific methodology. It enables researchers to build with confidence on the methods and findings of others, reuse and extend computational pipelines, and thereby drive scientific progress. Since many experimental studies rely on computational analyses, biologists need guidance on how to set up and document reproducible data analyses or simulations. In this paper, we address several questions about reproducibility. For example, what are the technical and non-technical barriers to reproducible computational studies? What opportunities and challenges do computational notebooks offer to overcome some of these barriers? What tools are available and how can they be used effectively? We have developed a set of rules to serve as a guide to scientists with a specific focus on computational notebook systems, such as Jupyter Notebooks, which have become a tool of choice for many applications. Notebooks combine detailed workflows with narrative text and visualization of results. Combined with software repositories and open source licensing, notebooks are powerful tools for transparent, collaborative, reproducible, and reusable data analyses.",arXiv,2018
59,P963,"Better Code, Better Sharing:On the Need of Analyzing Jupyter Notebooks",journalArticle,"By bringing together code, text, and examples, Jupyter notebooks have become one of the most popular means to produce scientific results in a productive and reproducible way. As many of the notebook authors are experts in their scientific fields, but laymen with respect to software engineering, one may ask questions on the quality of notebooks and their code. In a preliminary study, we experimentally demonstrate that Jupyter notebooks are inundated with poor quality code, e.g., not respecting recommended coding practices, or containing unused variables and deprecated functions. Considering the education nature of Jupyter notebooks, these poor coding practices as well as the lacks of quality control might be propagated into the next generation of developers. Hence, we argue that there is a strong need to programmatically analyze Jupyter notebooks, calling on our community to pay more attention to the reliability of Jupyter notebooks.",arXiv:1906.05234 [cs],2019
60,P1117,"Bioportainer Workbench: a versatile and user-friendly system that integrates implementation, management, and use of bioinformatics resources in Docker environments",journalArticle,"Background: The Docker project is providing a promising strategy for the development of virtualization systems in bioinformatics. However, implementation, management, and launching of Docker containers is not entirely trivial for users not fully familiarized with command line interfaces. This has prompted the development of graphical user interfaces to facilitate the interaction of inexperienced users with Docker environments. Results: We describe the BioPortainer Workbench, an integrated Docker system that assists inexperienced users in interacting with a bioinformatics-dedicated Docker environment at 3 main levels: (i) infrastructure, (ii) platform, and (iii) application. Conclusions: The BioPortainer Workbench represents a pioneering effort in developing a comprehensive and easy-to-use Docker platform focused on bioinformatics, which may greatly assist in the dissemination of Docker virtualization technology in this complex field of research.",GigaScience,2019
61,P2286,DEWE: A novel tool for executing differential expression RNA-Seq workflows in biomedical research,journalArticle,"Background\nTranscriptomics profiling aims to identify and quantify all transcripts present within a cell type or tissue at a particular state, and thus provide information on the genes expressed in specific experimental settings, differentiation or disease conditions. RNA-Seq technology is becoming the standard approach for such studies, but available analysis tools are often hard to install, configure and use by users without advanced bioinformatics skills.\nMethods\nWithin reason, DEWE aims to make RNA-Seq analysis as easy for non-proficient users as for experienced bioinformaticians. DEWE supports two well-established and widely used differential expression analysis workflows: using Bowtie2 or HISAT2 for sequence alignment; and, both applying StringTie for quantification, and Ballgown and edgeR for differential expression analysis. Also, it enables the tailored execution of individual tools as well as helps with the management and visualisation of differential expression results.\nResults\nDEWE provides a user-friendly interface designed to reduce the learning curve of less knowledgeable users while enabling analysis customisation and software extension by advanced users. Docker technology helps overcome installation and configuration hurdles. In addition, DEWE produces high quality and publication-ready outputs in the form of tab-delimited files and figures, as well as helps researchers with further analyses, such as pathway enrichment analysis.\nConclusions\nThe abilities of DEWE are exemplified here by practical application to a comparative analysis of monocytes and monocyte-derived dendritic cells, a study of clinical relevance. DEWE installers and documentation are freely available at https://www.sing-group.org/dewe.",Computers in Biology and Medicine,2019
62,P2490,"DNAscan: personal computer compatible NGS analysis, annotation and visualisation",journalArticle,"BackgroundNext Generation Sequencing (NGS) is a commonly used technology for studying the genetic basis of biological processes and it underpins the aspirations of precision medicine. However, there are significant challenges when dealing with NGS data. Firstly, a huge number of bioinformatics tools for a wide range of uses exist, therefore it is challenging to design an analysis pipeline. Secondly, NGS analysis is computationally intensive, requiring expensive infrastructure, and many medical and research centres do not have adequate high performance computing facilities and cloud computing is not always an option due to privacy and ownership issues. Finally, the interpretation of the results is not trivial and most available pipelines lack the utilities to favour this crucial step.ResultsWe have therefore developed a fast and efficient bioinformatics pipeline that allows for the analysis of DNA sequencing data, while requiring little computational effort and memory usage. DNAscan can analyse a whole exome sequencing sample in 1 h and a 40x whole genome sequencing sample in 13 h, on a midrange computer. The pipeline can look for single nucleotide variants, small indels, structural variants, repeat expansions and viral genetic material (or any other organism). Its results are annotated using a customisable variety of databases and are available for an on-the-fly visualisation with a local deployment of the gene.iobio platform. DNAscan is implemented in Python. Its code and documentation are available on GitHub: https://github.com/KHP-Informatics/DNAscan. Instructions for an easy and fast deployment with Docker and Singularity are also provided on GitHub.ConclusionsDNAscan is an extremely fast and computationally efficient pipeline for analysis, visualization and interpretation of NGS data. It is designed to provide a powerful and easy-to-use tool for applications in biomedical research and diagnostic medicine, at minimal computational cost. Its comprehensive approach will maximise the potential audience of users, bringing such analyses within the reach of non-specialist laboratories, and those from centres with limited funding available.",BMC Bioinformatics,2019
63,P2927,EWAS-Galaxy: a tools suite for population epigenetics integrated into Galaxy,journalArticle,"Background: Epigenome-wide association studies (EWAS) analyse genome-wide activity of epigenetic marks in cohorts of different individuals to find associations between epigenetic variation and phenotype. One of the most common technique used in EWAS studies is the Infinium Methylation Assay, which quantifies the DNA methylation level of over 450k loci. Although a number of bioinformatics tools have been developed to analyse the assay they require some programming skills and experience to use them. Results: We have developed a collection of user-friendly tools for the Galaxy platform for those without experience aimed at DNA methylation analysis using the Infinium Methylation Assay. Our tool suite is integrated into Galaxy (http://galaxyproject.org), web based platform. This allows users to analyse data from the Infinium Methylation Assay in the easiest possible way. Conclusions: The EWAS suite provides a group of integrated tools that combine analytical methods into a range of handy analysis pipelines. Our tool suite is available from the Galaxy test toolshed, GitHub repository and also as a Docker image. The aim of this project is to make EWAS analysis more flexible and accessible to everyone.",bioRxiv,2019
64,P4726,"miCloud: A Plug-n-Play, Extensible, On-Premises Bioinformatics Cloud for Seamless Execution of Complex Next-Generation Sequencing Data Analysis Pipelines",journalArticle,"The availability of low-cost small-factor sequencers, such as the Illumina MiSeq, MiniSeq, or iSeq, have paved the way for democratizing genomics sequencing, providing researchers in minority universities with access to the technology that was previously only affordable by institutions with large core facilities. However, these instruments are not bundled with software for performing bioinformatics data analysis, and the data analysis can be the main bottleneck for independent laboratories or even small clinical facilities that consider adopting genomic sequencing for medical applications. To address this issue, we have developedmiCloud, a bioinformatics platform that enables genomic data analysis through a fully featured data analysis cloud, which seamlessly integrates with genome sequencers over the local network. ThemiCloudcan be easily deployed without any prior bioinformatics expertise on any computing environment, from a laboratory computer workstation to a university computer cluster. Our platform not only provides access to a set of preconfigured RNA-Seq and CHIP-Seq bioinformatics pipelines, but also enables users to develop or install new preconfigured tools from the large selection available on open-source online Docker container repositories. ThemiCloudbuilt-in analysis pipelines are also integrated with the Visual Omics Explorer framework (Kim et al., 2016), which provides rich interactive visualizations and publication-ready graphics from the next-generation sequencing data. Ultimately, themiClouddemonstrates a bioinformatics approach that can be adopted in the field for standardizing genomic data analysis, similarly to the way molecular biology sample preparation kits have standardized laboratory operations.",Journal of Computational Biology,2019
65,P5009,NanoDJ: a Dockerized Jupyter notebook for interactive Oxford Nanopore MinION sequence manipulation and genome assembly,journalArticle,The Oxford Nanopore Technologies (ONT) MinION portable sequencer makes it possible to use cutting-edge genomic technologies in the field and the academic classroom.,BMC Bioinformatics,2019
66,P5147,nf-core: Community curated bioinformatics pipelines,journalArticle,"The standardization, portability, and reproducibility of analysis pipelines is a renowned problem within the bioinformatics community. Bioinformatic analysis pipelines are often designed for execution on-premise, and this inevitably leads to a level of customisation and integration that is only applicable to the local infrastructure. More notably, the software required to run these pipelines is also tightly coupled with the local compute environment, and this leads to poor pipeline portability, and reproducibility of the ensuing results - both of which are fundamental requirements for the validation of scientific findings. Here we introduce nf-core, a framework that provides a community-driven platform for the creation and development of best practice analysis pipelines written in the Nextflow language. Nextflow has built-in support for pipeline execution on most computational infrastructures, as well as automated deployment using container technologies such as Conda, Docker, and Singularity. Therefore, key obstacles in pipeline development such as portability, reproducibility, scalability and unified parallelism are inherently addressed by all nf-core pipelines. Furthermore, to ensure that new pipelines can be added seamlessly, and existing pipelines are able to inherit up-to-date functionality the nf-core community is actively developing a suite of tools that automate pipeline creation, testing, deployment and synchronization. The peer-review process during pipeline development ensures that best practices and common usage patterns are imposed and therefore, adhere to community guidelines. Our primary goal is to provide a community-driven platform for high-quality, excellent documented and reproducible bioinformatics pipelines that can be utilized across various institutions and research facilities.",bioRxiv,2019
67,P5544,Pixel: a content management platform for quantitative omics data,journalArticle,"Background In biology, high-throughput experimental technologies, also referred as “omics” technologies, are increasingly used in research laboratories. Several thousands of gene expression measurements can be obtained in a single experiment. Researchers are routinely facing the challenge to annotate, store, explore and mine all the biological information they have at their disposal. We present here the Pixel web application (Pixel Web App), an original content management platform to help people involved in a multi-omics biological project. Methods The Pixel Web App is built with open source technologies and hosted on the collaborative development platform GitHub (https://github.com/Candihub/pixel). It is written in Python using the Django framework and stores all the data in a PostgreSQL database. It is developed in the open and licensed under the BSD 3-clause license. The Pixel Web App is also heavily tested with both unit and functional tests, a strong code coverage and continuous integration provided by CircleCI. To ease the development and the deployment of the Pixel Web App, Docker and Docker Compose are used to bundle the application as well as its dependencies. Results The Pixel Web App offers researchers an intuitive way to annotate, store, explore and mine their multi-omics results. It can be installed on a personal computer or on a server to fit the needs of many users. In addition, anyone can enhance the application to better suit their needs, either by contributing directly on GitHub (encouraged) or by extending Pixel on their own. The Pixel Web App does not provide any computational programs to analyze the data. Still, it helps to rapidly explore and mine existing results and holds a strategic position in the management of research data.",PeerJ,2019
68,P6649,Successes and struggles with computational reproducibility: Lessons from the Fragile Families Challenge,journalArticle,"Reproducibility is fundamental to science, and an important component of reproducibility is computational reproducibility: the ability of a researcher to recreate the results in a published paper using the original author's raw data and code. Although most people agree that computational reproducibility is important, it is still difficult to achieve in practice. In this paper, we describe our approach to enabling computational reproducibility for the 12 papers in this special issue of Socius about the Fragile Families Challenge. Our approach draws on two tools commonly used by professional software engineers but not widely used by academic researchers: software containers (e.g., Docker) and cloud computing (e.g., Amazon Web Services). These tools enabled us to standardize the computing environment around each submission, which will ease computational reproducibility both today and in the future. Drawing on our successes and struggles, we conclude with recommendations to authors and journals.",SocArXiv,2019
69,P7319,Towards E ective Foraging by Data Scientists to Find Past Analysis Choices,conferencePaper,"Data scientists are responsible for the analysis decisions they make, but it is hard for them to track the process by which they achieved a result. Even when data scientists keep logs, it is onerous to make sense of the resulting large number of history records full of overlapping variants of code, output, plots, etc. We developed algorithmic and visualization techniques for notebook code environments to help data scientists forage for information in their history. To test these interventions, we conducted a think-aloud evaluation with 15 data scientists, where participants were asked to nd speci c information from the history of another person’s data science project. e participants succeed on a median of 80% of the tasks they performed. e quantitative results suggest promising aspects of our design, while qualitative results motivated a number of design improvements. e resulting system, called Verdant, is released as an open-source extension for JupyterLab.",,2019
70,P7320,Towards Effective Foraging by Data Scientists to Find Past Analysis Choices,conferencePaper,"Data scientists are responsible for the analysis decisions they make, but it is hard for them to track the process by which they achieved a result. Even when data scientists keep logs, it is onerous to make sense of the resulting large number of history records full of overlapping variants of code, output, plots, etc. We developed algorithmic and visualization techniques for notebook code environments to help data scientists forage for information in their history. To test these interventions, we conducted a think-aloud evaluation with 15 data scientists, where participants were asked to find specific information from the history of another person's data science project. The participants succeed on a median of 80% of the tasks they performed. The quantitative results suggest promising aspects of our design, while qualitative results motivated a number of design improvements. The resulting system, called Verdant, is released as an open-source extension for JupyterLab.",ACM,2019
